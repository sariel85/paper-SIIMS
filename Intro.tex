\externaldocument{paper.tex}

\section{Introduction}
	\label{sec:introduction}

	Making measurements has always been an integral part of the scientific method. It is through measurements and observations of various physical quantities that we can explore the surrounding world, allowing us to understand, make conclusions and even predictions about systems of interest. However, although measurements have such an integral role in our lives, we are usually not directly interested in the measurements themselves but rather in the properties or the state of a latent system which generates or causes these observed measurements, a system which might not be directly observable. For example when considering a radar system, we are not truly interested in the pattern of the electromagnetic wave received and measured by the antenna but rather in the location, size and velocity of an object in the antennas range whose reflectance shaped this wave.

	A latent system of interest can sometimes be observed in several different ways and data collected from it can be represented in various forms. In such cases we find that the specific observation and data representation methods used have a large influence on how effectively and easily we can make inferences about the latent system from the observed data. Ideally, the observation data should be represented in way in which its relation to the latent system is well understood and is as simple as possible, allowing it to be easily related to the actual system of interest. Not only the complexity of the connection between a system and its observations plays a role in the ability to analyze data but also the dimensionality of the observations. It is often the case that systems which are governed by a small number of variables (and are hence inherently low-dimensional in nature) are observed via redundant high-dimensional representations, producing data that lies on a low-dimensional manifold embedded in a high-dimensional ambient measurement space. Such unnecessarily high-dimensional representations obscure the low-dimensional nature of the system of interest and needlessly increase the computational cost of any algorithms applied to the observed data.
		
	When trying to tackle any signal processing or machine learning task, a considerable amount of focus is put initially on choosing or building an appropriate “convenient” representation of the observed data which would ideally be low-dimensional and simply relatable to the system of interest. This results in domain-specific knowledge being used to design hand-tailored and task-specific data representations, each used for a specific problem domain. While this practice has worked in some fields, it time consuming, tedious and sometimes impossible to follow due to insufficient pre-existing understanding of the latent system and its connection to the observed data. The availability of data generated by systems which are not sufficiently understood has become a commonly occurring situation with the massive increase in the ability to acquire, store and share data, leading to the rise of the concept of “Big-Data”. This trend has led to a reversal in the paradigm of tailoring data representations to a specific task based on prior knowledge, and nowadays the starting point for many interesting tasks is the existence of a large amount of data or measurements generated by systems which are not well understood. Much research in the fields of artificial intelligence, machine learning, data science and data mining has be devoted to methods for automatic learning directly from observed data in order to gain insights about the latent system. Representation learning and manifold learning methods \cite{bengio2013representation} attempt to learn representations from the available data, trying to lower the dimensionality of the data and uncover properties of the latent generating system. These methods have been proven to have the potential of improving existing data representations and discovering new representations in domains where little or no pre-existing knowledge is available. 

	Papers and work regarding representation and manifold learning learning usually address the relationship between the observed data and the structure of the representation learned from them. It can generally be said that most methods try to calculate some properties of the available observed data structure (these are often geometric properties) and then attempt to preserve them in the new learned representation \cite{tenenbaum2000global, perraultriemannian, bengio2013representation, coifman2006diffusion, donoho2003hessian, kamada1989algorithm, roweis2000nonlinear, saul2003think, singer2008non, tipping1999probabilistic}. This approach naturally arises from the desire to “lose” as little as possible information form the original data-set. This is especially clear for auto-encodes which explicitly require the the original observed data be completely retrievable from the new generated representation \cite{vincent2008extracting, vincent2010stacked, hinton2006reducing}. As a result of this approach which values the observed data over everything else, a considerable amount of effort is being invested in preservation of properties of the observed data. Unfortunately the way in which data is initially presented is often quite arbitrary and without a clear known connection to the latent system of actual interest. This becomes immediately clear if one considers the possibility that same system can produce very different observed data structures when using different measurement modalities. In light of this, it is sometimes clearly undesirable to invest such effort in preserving observed properties which might be arbitrary and irrelevant and one should instead try to seek and preserve intrinsic properties of the data, properties which are inherent to the latent system of interest and will manifest in any reasonable measurement or representation of the system. Unfortunately, most existing manifold learning methods do not address the issue of the relationship between the structure of the learned representation and some latent intrinsic structure or meaningful phenomena exhibited system of interest which generated the observed measurements. 
		
	This paper is organized as follows: In \cref{sec:motivation} we present a toy problem which serves as motivation for intrinsic and isometric manifold learning, in \cref{sec:setting} we formulate the problem mathematically. In \cref{sec:Intrinsic-isometric-manifold-learning} we present a novel manifold learning method, which is both intrinsic and isometric thus uncovering the geometric structure of the latent system of interest, however, we will notice that this methods requires knowledge of some local properties of the unknown observation function. In \cref{sec:Intrinsic-Metric-Estimation} we present a method to exploit observed measurements in order to robustly estimate the local properties of the observation function that were needed for the algorithm presented in \cref{sec:Intrinsic-isometric-manifold-learning}. In \cref{sec:results} we present the results of our suggested algorithm on a few synthetic data sets and also revisit the motivating example described in \cref{sec:motivation} and show that our approach indeed allows for intrinsic-isometric manifold learning in realistic conditions, enabling model invariant positioning and mapping. In \cref{sec:conclusions} we conclude the work, discuss a few key issues and a few possible future research directions.

\section{Toy problem and motivation}
	\label{sec:motivation}
	
	Our motivating example is that of mapping a 2-dimensional region and positioning of an agent within it base on observations made by the agent. Consider a 2-dimensional region which might represent the interior of a structure. We denote the set of points belonging to this region by $\mathcal{X}$ and the location of the agent by $\mathbf{x}$ as illustrated in \cref{fig:An-agent-in}.
		
	\begin{figure}[h]
		\begin{centering}
			\includegraphics[width=0.7\textwidth]{figures/Chapter_1/loc_example_intrinsic_shape}
			\par\end{centering}
		\caption{Agent in a $2$-dimensional space\label{fig:An-agent-in}}
	\end{figure}
	
	We are not able to directly observe the shape of $\mathcal{X}$ or the location of the agent $\mathbf{x}\in\mathcal{X}$, therefore they represent a latent system and variable. At each point $\mathbf{x}\in\mathcal{X}$ we can make measurements or observations which are functions of the location $\mathbf{x}$ at which the measurement was taken (for now we exclude the possibility of noise or other influencing variables). To allow visualization of this problem, we assume that 3 different measurements are made at each 2-dimensional location as described in. One might for example measure \ac{RSS} values at $\mathbf{x}$ from different transmitting antennas,a model that has been considered many times for the propose of internal positioning and localization systems \cite{ash2004sensor, bal2009localization, boukerche2007localization, costa2006distributed, kotaru2015spotfi, moses2003self, niculescu2003ad, patwari2003relative, ramadurai2003localization, yang2009indoor}.For the sake of simplicity and intuition, we further restrict ourselves to measurements decay as the agent is further away from a transmitting station (as is the case in the “free-space” model which corresponds to outdoor signal propagation). Such measurements are visualized in \cref{fig:First-measurement-modality} where the antenna symbol represent the location from which the signal originates and different colors represent different \ac{RSS} measurement values. One can also consider another set of 3 such measurements for which no known model is available as visualized \cref{fig:Second-measurement-modality}.


	\begin{figure}
		\centering
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=.5\linewidth]{figures/Chapter_1/sensor_1_all}
			\caption{First measurement modality\label{fig:First-measurement-modality}}
		\end{subfigure}%
		\begin{subfigure}{.5\textwidth}
			\centering
			\includegraphics[width=.5\linewidth]{figures/Chapter_1/sensor_2_all}
			\caption{Second measurement modality\label{fig:Second-measurement-modality}}
		\end{subfigure}
			\caption{Two different observation function modality\label{fig:test}}
	\end{figure}

		
		
	Although the system in both cases is observed via 3 different measurement values it is intuitively clear that it only has 2 inherit degrees of freedom, corresponding to the 2-dimensional location coordinates of the agent. If we look at many measurements taken at different locations, we get the structure of a 2-dimensional manifold embedded in 3-dimensional observation space as visualized in \cref{fig:Creation-of-observed} for the case of the \ac{RSS} measurements.
		
	\begin{figure}[h]
		\begin{centering}
			\includegraphics[width=0.7\textwidth]{figures/Chapter_1/measurment_setting}
			\par\end{centering}
		\caption{The observed manifold\label{fig:Creation-of-observed}}
	\end{figure}
		

	If sensor measurements are unique in $\mathcal{X}$ (i.e. no two locations within $\mathcal{X}$ correspond to the same set measurement values) one can ask whether we can infer the location $\mathbf{x}$ at which a certain measurement was made given its respective measurement value $\mathbf{y}=f\left(\mathbf{x}\right)$. Such inference would provide a localization of the agent based on the observed measurements. Applying this localization to many measurements obtained from different locations in $\mathcal{X}$ would provide a mapping of $\mathcal{X}$. If $f$ is known and the connection between the location and the measurement values is understood this becomes a classical problem of model-based localization, which is a specific case of a broader class of non-linear inverse problems \cite{engl2005nonlinear}. In such problems, the understanding of system is exploited in order to inverse it and retrieve the latent states of the system.
		
	However, as is often the case with signal propagation in indoor settings, the exact connection between the measurement values and the location of the agent is either unknown or is too complicated due to dependence on many factors which are unknown to us beforehand (such as room geometry, locations of walls, reflection, transmission, absorption of materials, etc.). As a result we cannot usually assume that the observation model $\mathbf{y}=f\left(\mathbf{x}\right)$ is known. This leads to a more challenging question: would it be possible to retrieve $\mathbf{x}$ from $f\mathbf{\left(\mathbf{x}\right)}$ without knowing $f$; we will call this problem “blind localization” or a “blind inverse problem”.
		
	Given that we find ourselves interested in the 2-dimensional structure of a set of measurements embedded in a 3-dimensional ambient space, one could typically try to address this as a manifold learning or dimensionality reduction problem. Unfortunately, application of manifold learning algorithms gives somewhat disappointing results as can be seen in \cref{fig:Application-of-manifold}. While some of these methods provide an interpretable low-dimensional parameterization of the latent space by preserving the basic topology of the manifold (points close to each other in the physical 2-dimensional space remain close to each other in the learned representation), non of them recover the true location of the agent or a proper structure preserving mapping of the intrinsic space.
	
	\begin{figure}[h]
		\centering{}%
		\begin{minipage}[t]{1\columnwidth}%
			\begin{center}
				\includegraphics[width=1\textwidth]{figures/Chapter_1/ml_total}
				\par\end{center}%
		\end{minipage}\caption{Manifold learning for ``blind'' localization and mapping \label{fig:Application-of-manifold}. The first row corresponds to the application on the manifold created via the observation function visualized in \cref{fig:First-measurement-modality}. The second row corresponds to the application on the manifold created via the observation function visualized in \cref{fig:Second-measurement-modality}. The bottom row represents application of manifold learning algorithm directly to the intrinsic manifold}
	\end{figure}

	This is even more evident if the same system is observed again via a set of measurements of a different nature. The measurement functions used for this second observation modality are visualized in. Here, the measurements do not have an intuitive interpretation of a possible physical measurement which better simulates situation where the measurement model is unknown.
		
		
	We notice that we get very different results for each method when compared to the results in \cref{fig:Application-of-manifold}. It is therefore clear that these methods learn and preserve the structure of the observed-measurement manifold but they fail to capture the structure of the intrinsic latent system “behind” the measurements. Not only does this fail to achieve our goal of localization, it also emphasizes the dependence of manifold learning on the way data is observed/measured and that different observations of the same latent system lead to different learned low dimensional representation. Even if we apply manifold learning methods directly to the latent variable $\mathbf{x}\in\mathcal{X}$ we still do not necessarily retrieve the structure of $\mathcal{X}$ as seen in \cref{fig:Application-of-manifold}. This is because not all manifold learning methods are isometric ,i.e., they do not preserve distances.	
		
	This example shows the inadequacy of existing manifold learning algorithms for solving problems where we are interested is recovering the intrinsic geometric structure of a latent system. 
	
	Existing manifold learning algorithms generate new representations of data manifolds while preserving cretin properties of the \textit{observed} manifold. This approach initially seems quite reasonable; however, as was shown in the motivating example given in \cref{sec:motivation}, this might not always be the best strategy. When data originates from an intrinsic low-dimensional latent system observed via an observation function, the observed manifold is affected by the specific (and often arbitrary) observation function used, as a result, the use of different observation modalities results in different generated representations when applying manifold learning methods. This fails to capture the intrinsic characteristics of the different observed manifolds which all originate from the same latent low-dimensional system. Such settings give rise to the need for manifold learning algorithms which are \textit{intrinsic}. Intrinsic in this context means that the learned representation should not depend on the observation function or sensor modality used. However, intrinsicness by itself is not enough in order to retrieve the latent low-dimensional geometric structure of the data and ,as the motivating example also showed, even if applied directly to the low-dimensional latent space (thus avoiding any dependence on a specific observation function), many existing manifold learning methods distort the geometric structure of the latent low-dimensional space, which in many cases bears a special significance and might be relatable to some meaningful physical properties. In order to explicitly preserve the geometric structure of the latent manifold, the learned representation needs to be \textit{isometric} (distance preserving) as well. 
		
	A manifold learning method, which is isometric with respect to the intrinsic structure hidden in data, would allow for the retrieval of $\mathbf{x}$ and $\mathcal{X}$ from observed data without requiring knowledge of the specific observation model. In the case of the localization and positioning this would be in contrast to existing internal positioning algorithms which mostly depend on developing complex models for indoor signal propagation.
	
	In this paper our goal is to present an approach for dimensionality reduction which achieves these two goals simultaneously, we do this by developing a manifold learning method that is \textit{isometric with respect to the latent intrinsic geometric structure}.
	
	
\section{Problem formulation}
	\label{sec:setting}

	Let ${\cal X}$ be a path-connected (i.e. each two points in $\mathcal{X}$ can be connected via a curve that does not leave $\mathcal{X}$) subset of $n$-dimensional Euclidean space $\mathbb{R}^{n}$. $\mathcal{X}$ is the intrinsic or latent manifold, $\mathbb{R}^{n}$ the intrinsic vector space and n the intrinsic dimension. Let $f:{\cal X}\rightarrow\mathbb{R}^{m}$ be a continuously-differentiable, injective function called the observation function. The observed manifold, which is the image of f when applied to ${\cal X}$, is denoted by ${\cal Y}=f\left({\cal X}\right)$. $\mathbb{R}^{m}$ is called the observation vector space and m the observation dimension. Since f is injective we have that $n \leq m$ (for many cases of interest involving high-dimensional observations we might even have that $n\ll m$).
		
	The intrinsic manifold ${\cal X}$ represents the set of all the possible latent states of a low-dimensional system. These states are not directly accessible (which justifies the use of the term latent) but are rather only indirectly observable via f which represents the measurements we are given for each intrinsic state ${\bf x}\in{\cal X}$. ${\cal Y}$ represents the set of all possible measurement/observation values on $\mathcal{X}$. In any realistic scenario we can only discuss finite sets of sample points from both $\mathcal{X}$ and ${\cal Y}$, therefore we also define sample subsets of these two manifolds: Let ${\cal X}_{s}=\left\{ {\bf x}_{i}\right\} _{i=1}^{N}$ be the intrinsic subset of N points sampled from $\mathcal{X}$, such that for any integer $i$ satisfying $1\leq i\leq N$ we have ${\bf x}_{i}\in{\cal X}_{s}\subseteq{\cal X}$. Points in ${\cal X}_{s}$ are denoted by ${\bf x}_{i}=\left(\begin{array}{ccccc} x_{i,1} & x_{i,2} & \ldots & \ldots & x_{i,n}\end{array}\right)$ where $x_{i,j}$ is the $j$-th entry of the $i$-th member of the the set ${\cal X}_{s}$. Analogously, ${\cal Y}_{s}=f\left({\cal X}_{s}\right)$ is the subset of all observation function values received by observing the intrinsic sampled subset ${\cal X}_{s}$. We call ${\cal Y}_{s}$ the observation sampled subset. Points in ${\cal Y}_{s}$ are denoted by $f\left(\mathbf{x}_{i}\right)={\bf y}_{i}=\left(\begin{array}{ccccc} y_{i,1} & y_{i,2} & \ldots & \ldots & y_{i,m}\end{array}\right)$ and are called the observed variable, where $y_{i,j}$ is the j-th entry of the i-th member of the the set ${\cal Y}_{s}$. We do not assume that the observation function $f$ is known
		
	To make the above terms more intuitive we present them with respect to the localization example given in \cref{sec:introduction} which will be discussed again in \cref{ssec:localization} . In this example the intrinsic vector space is the $2$-dimensional physical plane which is of intrinsic dimension $n=2$ and $\mathcal{X}$ represents the set of all locations that our agent can travel to within $\mathbb{R}^{2}$. f represents the observations we are able to make at each location, which depend solely on the location where the measurement was made. $\mathcal{Y}$ represents the set of all possible observation values. The requirement that $f$ be injective is clear in this context if we want to be able to map each measurement value to a unique corresponding location of the agent. 
		
	Under the setting just describe and given access only to the observation sampled subset $\mathcal{Y_{S}}$, we wish to generate a new embedding ${\cal \widetilde{X}}_{s}=\left\{ \tilde{{\bf x}}_{i}\right\} _{i=1}^{N}$ of the observed points ${\cal Y}_{s}=\left\{ {\bf y}_{i}\right\} _{i=1}^{N}$ into $n$-dimensional Euclidean space, which respects the Euclidean metric structure of the latent set $\mathcal{X}_{s}$ which can be viewed as a metric space with respect to the Euclidean distance in the intrinsic vector space $\mathbb{R}^{n}$:
	\[
	d_{euc}\left(\text{\ensuremath{\mathbf{x}}}_{i},\text{\ensuremath{\mathbf{x}}}_{j}\right)=\left\Vert {\bf x}_{i}-{\bf x}_{j}\right\Vert 
	\]	
	In order to give a quantitative measure of ``structure preservation'' and ``intrinsic-isometry'' of our constructed embedding, we utilize the well known stress function which is commonly used in \ac{LS-MDS}. This function penalizes for discrepancies between the inter-point Euclidean distances in the constructed embedding and some ideal distance or dissimilarity. In our case the ideal distances are the true Euclidean distances in the intrinsic space. This results in the following cost function for the embedding process:
	\begin{equation}
	\sigma_{stress}\left({\cal \widetilde{X}}_{s}\right)=\sum_{i<j}\left(\left\Vert \mathbf{\widetilde{x}}_{i}-\mathbf{\widetilde{x}}_{j}\right\Vert -\left\Vert {\bf x}_{i}-{\bf x}_{j}\right\Vert \right)^{2}\label{eq:stress}
	\end{equation}
	Lower stress values imply that the Euclidean structure of the embedding respects the intrinsic Euclidean geometry more. Our goal is to find a set of embedding points ${\cal \widetilde{X}}_{s}=\left\{ \tilde{{\bf x}}_{i}\right\} _{i=1}^{N}$that minimizes this cost function. We notice that the stress cost function depends only on inter-point distances and is therefore invariant to rigid rotations of the created embedding. This means that even in the best case scenario we can only expect to retrieve ${\cal X}_{s}$ up to an unknown rigid rotation. This however is insignificant in most practical cases since it is equivalent to an arbitrary rotation of the problem axis which does not have a practical effect on any signal processing or machine learning task.
	Although the values of observed measurements $\mathcal{Y_{S}}$ are known, the connection between them and the intrinsic states is hidden since the observation function $f$ is unknown, hence the structure of $\mathcal{X}_{S}$ cannot be directly inferred from $\mathcal{Y}_{S}$ therefore the main challenge in the goal we have presented is that the ground-truth inter-point Euclidean distances $\left\Vert {\bf x}_{i}-{\bf x}_{j}\right\Vert $, which we are trying to adhere to, are unknown and need to be approximated from the observed data. A second challenge is that the cost function presented in \cref{eq:stress} leads to a non-convex optimization problem which requires a ``good'' initial solution in order to converge to a local-minima which is close to the global minima.